{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f2090e-e519-44c5-a9d7-c085e5dc8170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vbocharnikov/.conda/envs/my_pyg/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm as tqdm\n",
    "# from tqdm import tqdm\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from misc.utils import divide_chunks\n",
    "from dataset.vocab import Vocabulary\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "log = logger\n",
    "\n",
    "\n",
    "class TransactionDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 mlm,\n",
    "                 user_ids=None,\n",
    "                 seq_len=10,\n",
    "                 num_bins=10,\n",
    "                 cached=True,\n",
    "                 root=\"./data/card/\",\n",
    "                 fname=\"card_trans\",\n",
    "                 vocab_dir=\"checkpoints\",\n",
    "                 fextension=\"\",\n",
    "                 nrows=None,\n",
    "                 flatten=False,\n",
    "                 stride=5,\n",
    "                 adap_thres=10 ** 8,\n",
    "                 return_labels=False,\n",
    "                 skip_user=False):\n",
    "\n",
    "        self.root = root\n",
    "        self.fname = fname\n",
    "        self.nrows = nrows\n",
    "        self.fextension = f'_{fextension}' if fextension else ''\n",
    "        self.cached = cached\n",
    "        self.user_ids = user_ids\n",
    "        self.return_labels = return_labels\n",
    "        self.skip_user = skip_user\n",
    "\n",
    "        self.mlm = mlm\n",
    "        self.trans_stride = stride\n",
    "\n",
    "        self.flatten = flatten\n",
    "\n",
    "        self.vocab = Vocabulary(adap_thres)\n",
    "        self.seq_len = seq_len\n",
    "        self.encoder_fit = {}\n",
    "\n",
    "        self.trans_table = None\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.window_label = []\n",
    "\n",
    "        self.ncols = None\n",
    "        self.num_bins = num_bins\n",
    "        self.encode_data()\n",
    "        self.init_vocab()\n",
    "        self.prepare_samples()\n",
    "        self.save_vocab(vocab_dir)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.flatten:\n",
    "            return_data = torch.tensor(self.data[index], dtype=torch.long)\n",
    "        else:\n",
    "            return_data = torch.tensor(self.data[index], dtype=torch.long).reshape(self.seq_len, -1)\n",
    "\n",
    "        if self.return_labels:\n",
    "            return_data = (return_data, torch.tensor(self.labels[index], dtype=torch.long))\n",
    "\n",
    "        return return_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def save_vocab(self, vocab_dir):\n",
    "        file_name = path.join(vocab_dir, f'vocab{self.fextension}.nb')\n",
    "        log.info(f\"saving vocab at {file_name}\")\n",
    "        self.vocab.save_vocab(file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def label_fit_transform(column, enc_type=\"label\"):\n",
    "        if enc_type == \"label\":\n",
    "            mfit = LabelEncoder()\n",
    "        else:\n",
    "            mfit = MinMaxScaler()\n",
    "        mfit.fit(column)\n",
    "\n",
    "        return mfit, mfit.transform(column)\n",
    "\n",
    "    @staticmethod\n",
    "    def timeEncoder(X):\n",
    "        X_hm = X['Time'].str.split(':', expand=True)\n",
    "        d = pd.to_datetime(dict(year=X['Year'], month=X['Month'], day=X['Day'], hour=X_hm[0], minute=X_hm[1])).astype(\n",
    "            int)\n",
    "        return pd.DataFrame(d)\n",
    "\n",
    "    @staticmethod\n",
    "    def amountEncoder(X):\n",
    "        amt = X.apply(lambda x: x[1:]).astype(float).apply(lambda amt: max(1, amt)).apply(math.log)\n",
    "        return pd.DataFrame(amt)\n",
    "\n",
    "    @staticmethod\n",
    "    def fraudEncoder(X):\n",
    "        fraud = (X == 'Yes').astype(int)\n",
    "        return pd.DataFrame(fraud)\n",
    "\n",
    "    @staticmethod\n",
    "    def nanNone(X):\n",
    "        return X.where(pd.notnull(X), 'None')\n",
    "\n",
    "    @staticmethod\n",
    "    def nanZero(X):\n",
    "        return X.where(pd.notnull(X), 0)\n",
    "\n",
    "    def _quantization_binning(self, data):\n",
    "        qtls = np.arange(0.0, 1.0 + 1 / self.num_bins, 1 / self.num_bins)\n",
    "        bin_edges = np.quantile(data, qtls, axis=0)  # (num_bins + 1, num_features)\n",
    "        bin_widths = np.diff(bin_edges, axis=0)\n",
    "        bin_centers = bin_edges[:-1] + bin_widths / 2  # ()\n",
    "        return bin_edges, bin_centers, bin_widths\n",
    "\n",
    "    def _quantize(self, inputs, bin_edges):\n",
    "        quant_inputs = np.zeros(inputs.shape[0])\n",
    "        for i, x in enumerate(inputs):\n",
    "            quant_inputs[i] = np.digitize(x, bin_edges)\n",
    "        quant_inputs = quant_inputs.clip(1, self.num_bins) - 1  # Clip edges\n",
    "        return quant_inputs\n",
    "\n",
    "    def user_level_data(self):\n",
    "        fname = path.join(self.root, f\"preprocessed/{self.fname}.user{self.fextension}.pkl\")\n",
    "        trans_data, trans_labels = [], []\n",
    "\n",
    "        if self.cached and path.isfile(fname):\n",
    "            log.info(f\"loading cached user level data from {fname}\")\n",
    "            cached_data = pickle.load(open(fname, \"rb\"))\n",
    "            trans_data = cached_data[\"trans\"]\n",
    "            trans_labels = cached_data[\"labels\"]\n",
    "            columns_names = cached_data[\"columns\"]\n",
    "\n",
    "        else:\n",
    "            unique_users = self.trans_table[\"User\"].unique()\n",
    "            columns_names = list(self.trans_table.columns)\n",
    "\n",
    "            for user in tqdm.tqdm(unique_users):\n",
    "                user_data = self.trans_table.loc[self.trans_table[\"User\"] == user]\n",
    "                user_trans, user_labels = [], []\n",
    "                for idx, row in user_data.iterrows():\n",
    "                    row = list(row)\n",
    "\n",
    "                    # assumption that user is first field\n",
    "                    skip_idx = 1 if self.skip_user else 0\n",
    "\n",
    "                    user_trans.extend(row[skip_idx:-1])\n",
    "                    user_labels.append(row[-1])\n",
    "\n",
    "                trans_data.append(user_trans)\n",
    "                trans_labels.append(user_labels)\n",
    "\n",
    "            if self.skip_user:\n",
    "                columns_names.remove(\"User\")\n",
    "\n",
    "            with open(fname, 'wb') as cache_file:\n",
    "                pickle.dump({\"trans\": trans_data, \"labels\": trans_labels, \"columns\": columns_names}, cache_file)\n",
    "\n",
    "        # convert to str\n",
    "        return trans_data, trans_labels, columns_names\n",
    "\n",
    "    def format_trans(self, trans_lst, column_names):\n",
    "        trans_lst = list(divide_chunks(trans_lst, len(self.vocab.field_keys) - 2))  # 2 to ignore isFraud and SPECIAL\n",
    "        user_vocab_ids = []\n",
    "\n",
    "        sep_id = self.vocab.get_id(self.vocab.sep_token, special_token=True)\n",
    "\n",
    "        for trans in trans_lst:\n",
    "            vocab_ids = []\n",
    "            for jdx, field in enumerate(trans):\n",
    "                vocab_id = self.vocab.get_id(field, column_names[jdx])\n",
    "                vocab_ids.append(vocab_id)\n",
    "\n",
    "            # TODO : need to handle ncols when sep is not added\n",
    "            if self.mlm:  # and self.flatten:  # only add [SEP] for BERT + flatten scenario\n",
    "                vocab_ids.append(sep_id)\n",
    "\n",
    "            user_vocab_ids.append(vocab_ids)\n",
    "\n",
    "        return user_vocab_ids\n",
    "\n",
    "    def prepare_samples(self):\n",
    "        log.info(\"preparing user level data...\")\n",
    "        trans_data, trans_labels, columns_names = self.user_level_data()\n",
    "\n",
    "        log.info(\"creating transaction samples with vocab\")\n",
    "        for user_idx in tqdm.tqdm(range(len(trans_data))):\n",
    "            user_row = trans_data[user_idx]\n",
    "            user_row_ids = self.format_trans(user_row, columns_names)\n",
    "\n",
    "            user_labels = trans_labels[user_idx]\n",
    "\n",
    "            bos_token = self.vocab.get_id(self.vocab.bos_token, special_token=True)  # will be used for GPT2\n",
    "            eos_token = self.vocab.get_id(self.vocab.eos_token, special_token=True)  # will be used for GPT2\n",
    "            for jdx in range(0, len(user_row_ids) - self.seq_len + 1, self.trans_stride):\n",
    "                ids = user_row_ids[jdx:(jdx + self.seq_len)]\n",
    "                ids = [idx for ids_lst in ids for idx in ids_lst]  # flattening\n",
    "                if not self.mlm and self.flatten:  # for GPT2, need to add [BOS] and [EOS] tokens\n",
    "                    ids = [bos_token] + ids + [eos_token]\n",
    "                self.data.append(ids)\n",
    "\n",
    "            for jdx in range(0, len(user_labels) - self.seq_len + 1, self.trans_stride):\n",
    "                ids = user_labels[jdx:(jdx + self.seq_len)]\n",
    "                self.labels.append(ids)\n",
    "\n",
    "                fraud = 0\n",
    "                if len(np.nonzero(ids)[0]) > 0:\n",
    "                    fraud = 1\n",
    "                self.window_label.append(fraud)\n",
    "\n",
    "        assert len(self.data) == len(self.labels)\n",
    "\n",
    "        '''\n",
    "            ncols = total fields - 1 (special tokens) - 1 (label)\n",
    "            if bert:\n",
    "                ncols += 1 (for sep)\n",
    "        '''\n",
    "        self.ncols = len(self.vocab.field_keys) - 2 + (1 if self.mlm else 0)\n",
    "        log.info(f\"ncols: {self.ncols}\")\n",
    "        log.info(f\"no of samples {len(self.data)}\")\n",
    "\n",
    "    def get_csv(self, fname):\n",
    "        data = pd.read_csv(fname, nrows=self.nrows)\n",
    "        if self.user_ids:\n",
    "            log.info(f'Filtering data by user ids list: {self.user_ids}...')\n",
    "            self.user_ids = map(int, self.user_ids)\n",
    "            data = data[data['User'].isin(self.user_ids)]\n",
    "\n",
    "        self.nrows = data.shape[0]\n",
    "        log.info(f\"read data : {data.shape}\")\n",
    "        return data\n",
    "\n",
    "    def write_csv(self, data, fname):\n",
    "        log.info(f\"writing to file {fname}\")\n",
    "        data.to_csv(fname, index=False)\n",
    "\n",
    "    def init_vocab(self):\n",
    "        column_names = list(self.trans_table.columns)\n",
    "        if self.skip_user:\n",
    "            column_names.remove(\"User\")\n",
    "\n",
    "        self.vocab.set_field_keys(column_names)\n",
    "\n",
    "        for column in column_names:\n",
    "            unique_values = self.trans_table[column].value_counts(sort=True).to_dict()  # returns sorted\n",
    "            for val in unique_values:\n",
    "                self.vocab.set_id(val, column)\n",
    "\n",
    "        log.info(f\"total columns: {list(column_names)}\")\n",
    "        log.info(f\"total vocabulary size: {len(self.vocab.id2token)}\")\n",
    "\n",
    "        for column in self.vocab.field_keys:\n",
    "            vocab_size = len(self.vocab.token2id[column])\n",
    "            log.info(f\"column : {column}, vocab size : {vocab_size}\")\n",
    "\n",
    "            if vocab_size > self.vocab.adap_thres:\n",
    "                log.info(f\"\\tsetting {column} for adaptive softmax\")\n",
    "                self.vocab.adap_sm_cols.add(column)\n",
    "\n",
    "    def encode_data(self):\n",
    "        dirname = path.join(self.root, \"preprocessed\")\n",
    "        fname = f'{self.fname}{self.fextension}.encoded.csv'\n",
    "        data_file = path.join(self.root, f\"{self.fname}.csv\")\n",
    "\n",
    "        if self.cached and path.isfile(path.join(dirname, fname)):\n",
    "            log.info(f\"cached encoded data is read from {fname}\")\n",
    "            self.trans_table = self.get_csv(path.join(dirname, fname))\n",
    "            encoder_fname = path.join(dirname, f'{self.fname}{self.fextension}.encoder_fit.pkl')\n",
    "            self.encoder_fit = pickle.load(open(encoder_fname, \"rb\"))\n",
    "            return\n",
    "\n",
    "        data = self.get_csv(data_file)\n",
    "        log.info(f\"{data_file} is read.\")\n",
    "\n",
    "        log.info(\"nan resolution.\")\n",
    "        data['Errors?'] = self.nanNone(data['Errors?'])\n",
    "        data['Is Fraud?'] = self.fraudEncoder(data['Is Fraud?'])\n",
    "        data['Zip'] = self.nanZero(data['Zip'])\n",
    "        data['Merchant State'] = self.nanNone(data['Merchant State'])\n",
    "        data['Use Chip'] = self.nanNone(data['Use Chip'])\n",
    "        data['Amount'] = self.amountEncoder(data['Amount'])\n",
    "\n",
    "        sub_columns = ['Errors?', 'MCC', 'Zip', 'Merchant State', 'Merchant City', 'Merchant Name', 'Use Chip']\n",
    "\n",
    "        log.info(\"label-fit-transform.\")\n",
    "        for col_name in tqdm.tqdm(sub_columns):\n",
    "            col_data = data[col_name]\n",
    "            col_fit, col_data = self.label_fit_transform(col_data)\n",
    "            self.encoder_fit[col_name] = col_fit\n",
    "            data[col_name] = col_data\n",
    "\n",
    "        log.info(\"timestamp fit transform\")\n",
    "        timestamp = self.timeEncoder(data[['Year', 'Month', 'Day', 'Time']])\n",
    "        timestamp_fit, timestamp = self.label_fit_transform(timestamp, enc_type=\"time\")\n",
    "        self.encoder_fit['Timestamp'] = timestamp_fit\n",
    "        data['Timestamp'] = timestamp\n",
    "\n",
    "        log.info(\"timestamp quant transform\")\n",
    "        coldata = np.array(data['Timestamp'])\n",
    "        bin_edges, bin_centers, bin_widths = self._quantization_binning(coldata)\n",
    "        data['Timestamp'] = self._quantize(coldata, bin_edges)\n",
    "        self.encoder_fit[\"Timestamp-Quant\"] = [bin_edges, bin_centers, bin_widths]\n",
    "\n",
    "        log.info(\"amount quant transform\")\n",
    "        coldata = np.array(data['Amount'])\n",
    "        bin_edges, bin_centers, bin_widths = self._quantization_binning(coldata)\n",
    "        data['Amount'] = self._quantize(coldata, bin_edges)\n",
    "        self.encoder_fit[\"Amount-Quant\"] = [bin_edges, bin_centers, bin_widths]\n",
    "\n",
    "        columns_to_select = ['User',\n",
    "                             'Card',\n",
    "                             'Timestamp',\n",
    "                             'Amount',\n",
    "                             'Use Chip',\n",
    "                             'Merchant Name',\n",
    "                             'Merchant City',\n",
    "                             'Merchant State',\n",
    "                             'Zip',\n",
    "                             'MCC',\n",
    "                             'Errors?',\n",
    "                             'Is Fraud?']\n",
    "\n",
    "        self.trans_table = data[columns_to_select]\n",
    "\n",
    "        log.info(f\"writing cached csv to {path.join(dirname, fname)}\")\n",
    "        if not path.exists(dirname):\n",
    "            os.mkdir(dirname)\n",
    "        self.write_csv(self.trans_table, path.join(dirname, fname))\n",
    "\n",
    "        encoder_fname = path.join(dirname, f'{self.fname}{self.fextension}.encoder_fit.pkl')\n",
    "        log.info(f\"writing cached encoder fit to {encoder_fname}\")\n",
    "        pickle.dump(self.encoder_fit, open(encoder_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a218c-fc5e-4e64-b606-03d3aa3c6fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888ad7c4-0d68-4261-9fc5-6353b044c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransactionFeatureDataset(Dataset):\n",
    "#     \"\"\"Transaction Feature Dataset for Fraud Detection task.\"\"\"\n",
    "\n",
    "#     def __init__(self, data, label, with_upsample=False):\n",
    "#         \"\"\"Args:\n",
    "#             - data: sample feature extracted from TabBERT.\n",
    "#             - label: label in sample (window) level.\n",
    "#             - with_upsample: if True, upsample fraudulent data to have the same amount with non-fraudulent data.\n",
    "#         \"\"\"\n",
    "#         self.data = data\n",
    "#         self.label = label\n",
    "#         if with_upsample:\n",
    "#             self._upsample()\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         return self.data[item], self.label[item]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def _upsample(self):\n",
    "#         logger.info('Upsample fraudulent samples.')\n",
    "#         non_fraud = self.data[self.label == 0]\n",
    "#         fraud = self.data[self.label == 1]\n",
    "#         fraud_upsample = resample(fraud, replace=True, n_samples=non_fraud.shape[0], random_state=2022)\n",
    "#         self.data = torch.cat((fraud_upsample, non_fraud))\n",
    "#         self.label = torch.cat((torch.ones(fraud_upsample.shape[0]), torch.zeros(non_fraud.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a524585-62f5-4ee1-b469-97382352e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/card/card_transaction.v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150606c4-7eeb-4728-90a0-c893adb893fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is Fraud?\n",
       "No     24357143\n",
       "Yes       29757\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Is Fraud?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd4413a-1ae3-4073-8d0b-a7e8b7d0fa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:04<00:00,  4.13it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = TransactionDataset(0, fname='card_transaction.v1', return_labels=True, stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5864a6-0a2a-45f6-9166-7ff3c06c0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from args import define_main_parser\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "from dataset.prsa import PRSADataset\n",
    "from dataset.card import TransactionDataset\n",
    "# from models.modules import TabFormerBertLM, TabFormerGPT2\n",
    "from misc.utils import random_split_dataset\n",
    "from dataset.datacollator import TransDataCollatorForLanguageModeling\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "log = logger\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3f03b6-de09-4480-859e-c86f76da6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 52\n",
    "random.seed(seed)  # python \n",
    "np.random.seed(seed)  # numpy\n",
    "torch.manual_seed(seed)  # torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)  # torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57b7b063-b0fd-4ddf-9b4f-3cad4bfb679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dataset.vocab\n",
    "custom_special_tokens = vocab.get_special_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c59b20a-21d7-40c7-80a5-6ba0432a15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, val, test [0.6. 0.2, 0.2]\n",
    "totalN = len(dataset)\n",
    "trainN = int(0.6 * totalN)\n",
    "\n",
    "valtestN = totalN - trainN\n",
    "valN = int(valtestN * 0.5)\n",
    "testN = valtestN - valN\n",
    "\n",
    "assert totalN == trainN + valN + testN\n",
    "\n",
    "lengths = [trainN, valN, testN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a86f34d-4609-4981-9d3f-7a5c79c6f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/12/2024 11:08:18 - INFO - __main__ -   # lengths: train [1462673]  valid [487558]  test [487558]\n",
      "09/12/2024 11:08:18 - INFO - __main__ -   # lengths: train [0.60]  valid [0.20]  test [0.20]\n"
     ]
    }
   ],
   "source": [
    "log.info(f\"# lengths: train [{trainN}]  valid [{valN}]  test [{testN}]\")\n",
    "log.info(\"# lengths: train [{:.2f}]  valid [{:.2f}]  test [{:.2f}]\".format(trainN / totalN, valN / totalN,\n",
    "                                                                           testN / totalN))\n",
    "\n",
    "train_dataset, eval_dataset, test_dataset = random_split_dataset(dataset, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "668e86b4-3aa9-4be6-8ea6-1609d9a0a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers.models.bert import BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import ACT2FN\n",
    "from transformers.models.bert.modeling_bert import BertForMaskedLM\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from models.custom_criterion import CustomAdaptiveLogSoftmax\n",
    "\n",
    "BertLayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "class TabFormerBertConfig(BertConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        flatten=True,\n",
    "        ncols=12,\n",
    "        vocab_size=30522,\n",
    "        field_hidden_size=64,\n",
    "        hidden_size=768,\n",
    "        num_attention_heads=12,\n",
    "        pad_token_id=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "        self.ncols = ncols\n",
    "        self.field_hidden_size = field_hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.flatten = flatten\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_attention_heads=num_attention_heads\n",
    "\n",
    "class TabFormerBertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.field_hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class TabFormerBertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = TabFormerBertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class TabFormerBertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = TabFormerBertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "\n",
    "class TabFormerBertForMaskedLM(BertForMaskedLM):\n",
    "    def __init__(self, config, vocab):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.cls = TabFormerBertOnlyMLMHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            masked_lm_labels=None,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            lm_labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]  # [bsz * seqlen * hidden]\n",
    "\n",
    "        if not self.config.flatten:\n",
    "            output_sz = list(sequence_output.size())\n",
    "            expected_sz = [output_sz[0], output_sz[1]*self.config.ncols, -1]\n",
    "            sequence_output = sequence_output.view(expected_sz)\n",
    "            masked_lm_labels = masked_lm_labels.view(expected_sz[0], -1)\n",
    "\n",
    "        prediction_scores = self.cls(sequence_output) # [bsz * seqlen * vocab_sz]\n",
    "\n",
    "        outputs = (prediction_scores,) + outputs[2:]\n",
    "\n",
    "        # prediction_scores : [bsz x seqlen x vsz]\n",
    "        # masked_lm_labels  : [bsz x seqlen]\n",
    "\n",
    "        total_masked_lm_loss = 0\n",
    "\n",
    "        seq_len = prediction_scores.size(1)\n",
    "        # TODO : remove_target is True for card\n",
    "        field_names = self.vocab.get_field_keys(remove_target=True, ignore_special=False)\n",
    "        for field_idx, field_name in enumerate(field_names):\n",
    "            col_ids = list(range(field_idx, seq_len, len(field_names)))\n",
    "\n",
    "            global_ids_field = self.vocab.get_field_ids(field_name)\n",
    "\n",
    "            prediction_scores_field = prediction_scores[:, col_ids, :][:, :, global_ids_field]  # bsz * 10 * K\n",
    "            masked_lm_labels_field = masked_lm_labels[:, col_ids]\n",
    "            masked_lm_labels_field_local = self.vocab.get_from_global_ids(global_ids=masked_lm_labels_field,\n",
    "                                                                          what_to_get='local_ids')\n",
    "\n",
    "            nfeas = len(global_ids_field)\n",
    "            loss_fct = self.get_criterion(field_name, nfeas, prediction_scores.device)\n",
    "\n",
    "            masked_lm_loss_field = loss_fct(prediction_scores_field.view(-1, len(global_ids_field)),\n",
    "                                            masked_lm_labels_field_local.view(-1))\n",
    "\n",
    "            total_masked_lm_loss += masked_lm_loss_field\n",
    "\n",
    "        return (total_masked_lm_loss,) + outputs\n",
    "\n",
    "    def get_criterion(self, fname, vs, device, cutoffs=False, div_value=4.0):\n",
    "\n",
    "        if fname in self.vocab.adap_sm_cols:\n",
    "            if not cutoffs:\n",
    "                cutoffs = [int(vs/15), 3*int(vs/15), 6*int(vs/15)]\n",
    "\n",
    "            criteria = CustomAdaptiveLogSoftmax(in_features=vs, n_classes=vs, cutoffs=cutoffs, div_value=div_value)\n",
    "\n",
    "            return criteria.to(device)\n",
    "        else:\n",
    "            return CrossEntropyLoss()\n",
    "\n",
    "class TabFormerBertModel(BertForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.cls = TabFormerBertOnlyMLMHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            masked_lm_labels=None,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            lm_labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]  # [bsz * seqlen * hidden]\n",
    "\n",
    "        return sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e01ffaa5-636e-4ec0-bed5-1ee0f9a3d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabFormerBertLM:\n",
    "    def __init__(self, special_tokens, vocab, field_ce=False, flatten=False, ncols=None, field_hidden_size=768):\n",
    "\n",
    "        self.ncols = ncols\n",
    "        self.vocab = vocab\n",
    "        vocab_file = self.vocab.filename\n",
    "        hidden_size = field_hidden_size if flatten else (field_hidden_size * self.ncols)\n",
    "\n",
    "        self.config = TabFormerBertConfig(vocab_size=len(self.vocab),\n",
    "                                          ncols=self.ncols,\n",
    "                                          hidden_size=hidden_size,\n",
    "                                          field_hidden_size=field_hidden_size,\n",
    "                                          flatten=flatten,\n",
    "                                          num_attention_heads=self.ncols)\n",
    "\n",
    "        self.tokenizer = BertTokenizer(vocab_file,\n",
    "                                       do_lower_case=False,\n",
    "                                       **special_tokens)\n",
    "        self.model = self.get_model(field_ce, flatten)\n",
    "\n",
    "    def get_model(self, field_ce, flatten):\n",
    "\n",
    "        model = TabFormerBertForMaskedLM(self.config, self.vocab)\n",
    "        # if flatten and not field_ce:\n",
    "        #     # flattened vanilla BERT\n",
    "        #     model = BertForMaskedLM(self.config)\n",
    "        # elif flatten and field_ce:\n",
    "        #     # flattened field CE BERT\n",
    "        #     model = TabFormerBertForMaskedLM(self.config, self.vocab)\n",
    "        # else:\n",
    "        #     # hierarchical field CE BERT\n",
    "        #     model = TabFormerHierarchicalLM(self.config, self.vocab)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe70abc6-f4f1-4a1f-bd31-338d0471f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "class TabFormerTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unk_token=\"<|endoftext|>\",\n",
    "        bos_token=\"<|endoftext|>\",\n",
    "        eos_token=\"<|endoftext|>\",\n",
    "    ):\n",
    "\n",
    "        super().__init__(bos_token=bos_token, eos_token=eos_token, unk_token=unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ee743e-d9dd-4124-a3e4-fec137a8509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_net = TabFormerBertLM(custom_special_tokens,\n",
    "                               vocab=vocab,\n",
    "                               # field_ce=args.field_ce,\n",
    "                               # flatten=args.flatten,\n",
    "                               ncols=dataset.ncols,\n",
    "                               # field_hidden_size=args.field_hs\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ccf478d-084b-4e12-9859-a8e83087d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tab_net.get_model(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43d31b76-0638-49a1-bcff-2de609d9a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = eval(\"DataCollatorForLanguageModeling\")(\n",
    "        tokenizer=tab_net.tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e8b92aa-2c19-4e94-afe7-c1a8cd8337c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments('out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb77ba65-bada-409a-a007-9895315e3e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/12/2024 11:12:01 - WARNING - accelerate.utils.other -   Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=tab_net.model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "986dbf83-209f-485b-a050-b897a6d89ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "epochs = 4\n",
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "train_acc_ = []\n",
    "test_acc_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8156002b-6078-43e0-94f8-f7efa8f8cbf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find a valid checkpoint at models/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/my_pyg/lib/python3.12/site-packages/transformers/trainer.py:1857\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_from_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_enabled:\n\u001b[0;32m-> 1857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_from_checkpoint(resume_from_checkpoint)\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;66;03m# In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrainerState\u001b[38;5;241m.\u001b[39mload_from_json(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(resume_from_checkpoint, TRAINER_STATE_NAME))\n",
      "File \u001b[0;32m~/.conda/envs/my_pyg/lib/python3.12/site-packages/transformers/trainer.py:2459\u001b[0m, in \u001b[0;36mTrainer._load_from_checkpoint\u001b[0;34m(self, resume_from_checkpoint, model)\u001b[0m\n\u001b[1;32m   2442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_from_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is only supported when using PyTorch FSDP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   2446\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(f)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m adapter_subdirs\n\u001b[1;32m   2458\u001b[0m ):\n\u001b[0;32m-> 2459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a valid checkpoint at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_from_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2461\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_from_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(config_file):\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find a valid checkpoint at models/"
     ]
    }
   ],
   "source": [
    "trainer.train(model_path='models/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-my_pyg]",
   "language": "python",
   "name": "conda-env-.conda-my_pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
