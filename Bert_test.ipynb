{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f2090e-e519-44c5-a9d7-c085e5dc8170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vbocharnikov/.conda/envs/my_pyg/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm as tqdm\n",
    "# from tqdm import tqdm\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from misc.utils import divide_chunks\n",
    "from dataset.vocab import Vocabulary\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "log = logger\n",
    "\n",
    "\n",
    "class TransactionDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 mlm,\n",
    "                 user_ids=None,\n",
    "                 seq_len=10,\n",
    "                 num_bins=10,\n",
    "                 cached=True,\n",
    "                 root=\"./data/card/\",\n",
    "                 fname=\"card_trans\",\n",
    "                 vocab_dir=\"checkpoints\",\n",
    "                 fextension=\"\",\n",
    "                 nrows=None,\n",
    "                 flatten=False,\n",
    "                 stride=5,\n",
    "                 adap_thres=10 ** 8,\n",
    "                 return_labels=False,\n",
    "                 skip_user=False):\n",
    "\n",
    "        self.root = root\n",
    "        self.fname = fname\n",
    "        self.nrows = nrows\n",
    "        self.fextension = f'_{fextension}' if fextension else ''\n",
    "        self.cached = cached\n",
    "        self.user_ids = user_ids\n",
    "        self.return_labels = return_labels\n",
    "        self.skip_user = skip_user\n",
    "\n",
    "        self.mlm = mlm\n",
    "        self.trans_stride = stride\n",
    "\n",
    "        self.flatten = flatten\n",
    "\n",
    "        self.vocab = Vocabulary(adap_thres)\n",
    "        self.seq_len = seq_len\n",
    "        self.encoder_fit = {}\n",
    "\n",
    "        self.trans_table = None\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.window_label = []\n",
    "\n",
    "        self.ncols = None\n",
    "        self.num_bins = num_bins\n",
    "        self.encode_data()\n",
    "        self.init_vocab()\n",
    "        self.prepare_samples()\n",
    "        self.save_vocab(vocab_dir)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.flatten:\n",
    "            return_data = torch.tensor(self.data[index], dtype=torch.long)\n",
    "        else:\n",
    "            return_data = torch.tensor(self.data[index], dtype=torch.long).reshape(self.seq_len, -1)\n",
    "\n",
    "        if self.return_labels:\n",
    "            return_data = (return_data, torch.tensor(self.labels[index], dtype=torch.long))\n",
    "\n",
    "        return return_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def save_vocab(self, vocab_dir):\n",
    "        file_name = path.join(vocab_dir, f'vocab{self.fextension}.nb')\n",
    "        log.info(f\"saving vocab at {file_name}\")\n",
    "        self.vocab.save_vocab(file_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def label_fit_transform(column, enc_type=\"label\"):\n",
    "        if enc_type == \"label\":\n",
    "            mfit = LabelEncoder()\n",
    "        else:\n",
    "            mfit = MinMaxScaler()\n",
    "        mfit.fit(column)\n",
    "\n",
    "        return mfit, mfit.transform(column)\n",
    "\n",
    "    @staticmethod\n",
    "    def timeEncoder(X):\n",
    "        X_hm = X['Time'].str.split(':', expand=True)\n",
    "        d = pd.to_datetime(dict(year=X['Year'], month=X['Month'], day=X['Day'], hour=X_hm[0], minute=X_hm[1])).astype(\n",
    "            int)\n",
    "        return pd.DataFrame(d)\n",
    "\n",
    "    @staticmethod\n",
    "    def amountEncoder(X):\n",
    "        amt = X.apply(lambda x: x[1:]).astype(float).apply(lambda amt: max(1, amt)).apply(math.log)\n",
    "        return pd.DataFrame(amt)\n",
    "\n",
    "    @staticmethod\n",
    "    def fraudEncoder(X):\n",
    "        fraud = (X == 'Yes').astype(int)\n",
    "        return pd.DataFrame(fraud)\n",
    "\n",
    "    @staticmethod\n",
    "    def nanNone(X):\n",
    "        return X.where(pd.notnull(X), 'None')\n",
    "\n",
    "    @staticmethod\n",
    "    def nanZero(X):\n",
    "        return X.where(pd.notnull(X), 0)\n",
    "\n",
    "    def _quantization_binning(self, data):\n",
    "        qtls = np.arange(0.0, 1.0 + 1 / self.num_bins, 1 / self.num_bins)\n",
    "        bin_edges = np.quantile(data, qtls, axis=0)  # (num_bins + 1, num_features)\n",
    "        bin_widths = np.diff(bin_edges, axis=0)\n",
    "        bin_centers = bin_edges[:-1] + bin_widths / 2  # ()\n",
    "        return bin_edges, bin_centers, bin_widths\n",
    "\n",
    "    def _quantize(self, inputs, bin_edges):\n",
    "        quant_inputs = np.zeros(inputs.shape[0])\n",
    "        for i, x in enumerate(inputs):\n",
    "            quant_inputs[i] = np.digitize(x, bin_edges)\n",
    "        quant_inputs = quant_inputs.clip(1, self.num_bins) - 1  # Clip edges\n",
    "        return quant_inputs\n",
    "\n",
    "    def user_level_data(self):\n",
    "        fname = path.join(self.root, f\"preprocessed/{self.fname}.user{self.fextension}.pkl\")\n",
    "        trans_data, trans_labels = [], []\n",
    "\n",
    "        if self.cached and path.isfile(fname):\n",
    "            log.info(f\"loading cached user level data from {fname}\")\n",
    "            cached_data = pickle.load(open(fname, \"rb\"))\n",
    "            trans_data = cached_data[\"trans\"]\n",
    "            trans_labels = cached_data[\"labels\"]\n",
    "            columns_names = cached_data[\"columns\"]\n",
    "\n",
    "        else:\n",
    "            unique_users = self.trans_table[\"User\"].unique()\n",
    "            columns_names = list(self.trans_table.columns)\n",
    "\n",
    "            for user in tqdm.tqdm(unique_users):\n",
    "                user_data = self.trans_table.loc[self.trans_table[\"User\"] == user]\n",
    "                user_trans, user_labels = [], []\n",
    "                for idx, row in user_data.iterrows():\n",
    "                    row = list(row)\n",
    "\n",
    "                    # assumption that user is first field\n",
    "                    skip_idx = 1 if self.skip_user else 0\n",
    "\n",
    "                    user_trans.extend(row[skip_idx:-1])\n",
    "                    user_labels.append(row[-1])\n",
    "\n",
    "                trans_data.append(user_trans)\n",
    "                trans_labels.append(user_labels)\n",
    "\n",
    "            if self.skip_user:\n",
    "                columns_names.remove(\"User\")\n",
    "\n",
    "            with open(fname, 'wb') as cache_file:\n",
    "                pickle.dump({\"trans\": trans_data, \"labels\": trans_labels, \"columns\": columns_names}, cache_file)\n",
    "\n",
    "        # convert to str\n",
    "        return trans_data, trans_labels, columns_names\n",
    "\n",
    "    def format_trans(self, trans_lst, column_names):\n",
    "        trans_lst = list(divide_chunks(trans_lst, len(self.vocab.field_keys) - 2))  # 2 to ignore isFraud and SPECIAL\n",
    "        user_vocab_ids = []\n",
    "\n",
    "        sep_id = self.vocab.get_id(self.vocab.sep_token, special_token=True)\n",
    "\n",
    "        for trans in trans_lst:\n",
    "            vocab_ids = []\n",
    "            for jdx, field in enumerate(trans):\n",
    "                vocab_id = self.vocab.get_id(field, column_names[jdx])\n",
    "                vocab_ids.append(vocab_id)\n",
    "\n",
    "            # TODO : need to handle ncols when sep is not added\n",
    "            if self.mlm:  # and self.flatten:  # only add [SEP] for BERT + flatten scenario\n",
    "                vocab_ids.append(sep_id)\n",
    "\n",
    "            user_vocab_ids.append(vocab_ids)\n",
    "\n",
    "        return user_vocab_ids\n",
    "\n",
    "    def prepare_samples(self):\n",
    "        log.info(\"preparing user level data...\")\n",
    "        trans_data, trans_labels, columns_names = self.user_level_data()\n",
    "\n",
    "        log.info(\"creating transaction samples with vocab\")\n",
    "        for user_idx in tqdm.tqdm(range(len(trans_data))):\n",
    "            user_row = trans_data[user_idx]\n",
    "            user_row_ids = self.format_trans(user_row, columns_names)\n",
    "\n",
    "            user_labels = trans_labels[user_idx]\n",
    "\n",
    "            bos_token = self.vocab.get_id(self.vocab.bos_token, special_token=True)  # will be used for GPT2\n",
    "            eos_token = self.vocab.get_id(self.vocab.eos_token, special_token=True)  # will be used for GPT2\n",
    "            for jdx in range(0, len(user_row_ids) - self.seq_len + 1, self.trans_stride):\n",
    "                ids = user_row_ids[jdx:(jdx + self.seq_len)]\n",
    "                ids = [idx for ids_lst in ids for idx in ids_lst]  # flattening\n",
    "                if not self.mlm and self.flatten:  # for GPT2, need to add [BOS] and [EOS] tokens\n",
    "                    ids = [bos_token] + ids + [eos_token]\n",
    "                self.data.append(ids)\n",
    "\n",
    "            for jdx in range(0, len(user_labels) - self.seq_len + 1, self.trans_stride):\n",
    "                ids = user_labels[jdx:(jdx + self.seq_len)]\n",
    "                self.labels.append(ids)\n",
    "\n",
    "                fraud = 0\n",
    "                if len(np.nonzero(ids)[0]) > 0:\n",
    "                    fraud = 1\n",
    "                self.window_label.append(fraud)\n",
    "\n",
    "        assert len(self.data) == len(self.labels)\n",
    "\n",
    "        '''\n",
    "            ncols = total fields - 1 (special tokens) - 1 (label)\n",
    "            if bert:\n",
    "                ncols += 1 (for sep)\n",
    "        '''\n",
    "        self.ncols = len(self.vocab.field_keys) - 2 + (1 if self.mlm else 0)\n",
    "        log.info(f\"ncols: {self.ncols}\")\n",
    "        log.info(f\"no of samples {len(self.data)}\")\n",
    "\n",
    "    def get_csv(self, fname):\n",
    "        data = pd.read_csv(fname, nrows=self.nrows)\n",
    "        if self.user_ids:\n",
    "            log.info(f'Filtering data by user ids list: {self.user_ids}...')\n",
    "            self.user_ids = map(int, self.user_ids)\n",
    "            data = data[data['User'].isin(self.user_ids)]\n",
    "\n",
    "        self.nrows = data.shape[0]\n",
    "        log.info(f\"read data : {data.shape}\")\n",
    "        return data\n",
    "\n",
    "    def write_csv(self, data, fname):\n",
    "        log.info(f\"writing to file {fname}\")\n",
    "        data.to_csv(fname, index=False)\n",
    "\n",
    "    def init_vocab(self):\n",
    "        column_names = list(self.trans_table.columns)\n",
    "        if self.skip_user:\n",
    "            column_names.remove(\"User\")\n",
    "\n",
    "        self.vocab.set_field_keys(column_names)\n",
    "\n",
    "        for column in column_names:\n",
    "            unique_values = self.trans_table[column].value_counts(sort=True).to_dict()  # returns sorted\n",
    "            for val in unique_values:\n",
    "                self.vocab.set_id(val, column)\n",
    "\n",
    "        log.info(f\"total columns: {list(column_names)}\")\n",
    "        log.info(f\"total vocabulary size: {len(self.vocab.id2token)}\")\n",
    "\n",
    "        for column in self.vocab.field_keys:\n",
    "            vocab_size = len(self.vocab.token2id[column])\n",
    "            log.info(f\"column : {column}, vocab size : {vocab_size}\")\n",
    "\n",
    "            if vocab_size > self.vocab.adap_thres:\n",
    "                log.info(f\"\\tsetting {column} for adaptive softmax\")\n",
    "                self.vocab.adap_sm_cols.add(column)\n",
    "\n",
    "    def encode_data(self):\n",
    "        dirname = path.join(self.root, \"preprocessed\")\n",
    "        fname = f'{self.fname}{self.fextension}.encoded.csv'\n",
    "        data_file = path.join(self.root, f\"{self.fname}.csv\")\n",
    "\n",
    "        if self.cached and path.isfile(path.join(dirname, fname)):\n",
    "            log.info(f\"cached encoded data is read from {fname}\")\n",
    "            self.trans_table = self.get_csv(path.join(dirname, fname))\n",
    "            encoder_fname = path.join(dirname, f'{self.fname}{self.fextension}.encoder_fit.pkl')\n",
    "            self.encoder_fit = pickle.load(open(encoder_fname, \"rb\"))\n",
    "            return\n",
    "\n",
    "        data = self.get_csv(data_file)\n",
    "        log.info(f\"{data_file} is read.\")\n",
    "\n",
    "        log.info(\"nan resolution.\")\n",
    "        data['Errors?'] = self.nanNone(data['Errors?'])\n",
    "        data['Is Fraud?'] = self.fraudEncoder(data['Is Fraud?'])\n",
    "        data['Zip'] = self.nanZero(data['Zip'])\n",
    "        data['Merchant State'] = self.nanNone(data['Merchant State'])\n",
    "        data['Use Chip'] = self.nanNone(data['Use Chip'])\n",
    "        data['Amount'] = self.amountEncoder(data['Amount'])\n",
    "\n",
    "        sub_columns = ['Errors?', 'MCC', 'Zip', 'Merchant State', 'Merchant City', 'Merchant Name', 'Use Chip']\n",
    "\n",
    "        log.info(\"label-fit-transform.\")\n",
    "        for col_name in tqdm.tqdm(sub_columns):\n",
    "            col_data = data[col_name]\n",
    "            col_fit, col_data = self.label_fit_transform(col_data)\n",
    "            self.encoder_fit[col_name] = col_fit\n",
    "            data[col_name] = col_data\n",
    "\n",
    "        log.info(\"timestamp fit transform\")\n",
    "        timestamp = self.timeEncoder(data[['Year', 'Month', 'Day', 'Time']])\n",
    "        timestamp_fit, timestamp = self.label_fit_transform(timestamp, enc_type=\"time\")\n",
    "        self.encoder_fit['Timestamp'] = timestamp_fit\n",
    "        data['Timestamp'] = timestamp\n",
    "\n",
    "        log.info(\"timestamp quant transform\")\n",
    "        coldata = np.array(data['Timestamp'])\n",
    "        bin_edges, bin_centers, bin_widths = self._quantization_binning(coldata)\n",
    "        data['Timestamp'] = self._quantize(coldata, bin_edges)\n",
    "        self.encoder_fit[\"Timestamp-Quant\"] = [bin_edges, bin_centers, bin_widths]\n",
    "\n",
    "        log.info(\"amount quant transform\")\n",
    "        coldata = np.array(data['Amount'])\n",
    "        bin_edges, bin_centers, bin_widths = self._quantization_binning(coldata)\n",
    "        data['Amount'] = self._quantize(coldata, bin_edges)\n",
    "        self.encoder_fit[\"Amount-Quant\"] = [bin_edges, bin_centers, bin_widths]\n",
    "\n",
    "        columns_to_select = ['User',\n",
    "                             'Card',\n",
    "                             'Timestamp',\n",
    "                             'Amount',\n",
    "                             'Use Chip',\n",
    "                             'Merchant Name',\n",
    "                             'Merchant City',\n",
    "                             'Merchant State',\n",
    "                             'Zip',\n",
    "                             'MCC',\n",
    "                             'Errors?',\n",
    "                             'Is Fraud?']\n",
    "\n",
    "        self.trans_table = data[columns_to_select]\n",
    "\n",
    "        log.info(f\"writing cached csv to {path.join(dirname, fname)}\")\n",
    "        if not path.exists(dirname):\n",
    "            os.mkdir(dirname)\n",
    "        self.write_csv(self.trans_table, path.join(dirname, fname))\n",
    "\n",
    "        encoder_fname = path.join(dirname, f'{self.fname}{self.fextension}.encoder_fit.pkl')\n",
    "        log.info(f\"writing cached encoder fit to {encoder_fname}\")\n",
    "        pickle.dump(self.encoder_fit, open(encoder_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a218c-fc5e-4e64-b606-03d3aa3c6fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "888ad7c4-0d68-4261-9fc5-6353b044c96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransactionFeatureDataset(Dataset):\n",
    "#     \"\"\"Transaction Feature Dataset for Fraud Detection task.\"\"\"\n",
    "\n",
    "#     def __init__(self, data, label, with_upsample=False):\n",
    "#         \"\"\"Args:\n",
    "#             - data: sample feature extracted from TabBERT.\n",
    "#             - label: label in sample (window) level.\n",
    "#             - with_upsample: if True, upsample fraudulent data to have the same amount with non-fraudulent data.\n",
    "#         \"\"\"\n",
    "#         self.data = data\n",
    "#         self.label = label\n",
    "#         if with_upsample:\n",
    "#             self._upsample()\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         return self.data[item], self.label[item]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def _upsample(self):\n",
    "#         logger.info('Upsample fraudulent samples.')\n",
    "#         non_fraud = self.data[self.label == 0]\n",
    "#         fraud = self.data[self.label == 1]\n",
    "#         fraud_upsample = resample(fraud, replace=True, n_samples=non_fraud.shape[0], random_state=2022)\n",
    "#         self.data = torch.cat((fraud_upsample, non_fraud))\n",
    "#         self.label = torch.cat((torch.ones(fraud_upsample.shape[0]), torch.zeros(non_fraud.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a524585-62f5-4ee1-b469-97382352e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/card/card_transaction.v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "150606c4-7eeb-4728-90a0-c893adb893fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Is Fraud?\n",
       "No     24357143\n",
       "Yes       29757\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Is Fraud?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd4413a-1ae3-4073-8d0b-a7e8b7d0fa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [08:04<00:00,  4.13it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = TransactionDataset(0, fname='card_transaction.v1', return_labels=True, stride=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5864a6-0a2a-45f6-9166-7ff3c06c0354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import join\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from args import define_main_parser\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "from dataset.prsa import PRSADataset\n",
    "from dataset.card import TransactionDataset\n",
    "# from models.modules import TabFormerBertLM, TabFormerGPT2\n",
    "from misc.utils import random_split_dataset\n",
    "from dataset.datacollator import TransDataCollatorForLanguageModeling\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "log = logger\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3f03b6-de09-4480-859e-c86f76da6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 52\n",
    "random.seed(seed)  # python \n",
    "np.random.seed(seed)  # numpy\n",
    "torch.manual_seed(seed)  # torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)  # torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57b7b063-b0fd-4ddf-9b4f-3cad4bfb679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dataset.vocab\n",
    "custom_special_tokens = vocab.get_special_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c59b20a-21d7-40c7-80a5-6ba0432a15d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, val, test [0.6. 0.2, 0.2]\n",
    "totalN = len(dataset)\n",
    "trainN = int(0.6 * totalN)\n",
    "\n",
    "valtestN = totalN - trainN\n",
    "valN = int(valtestN * 0.5)\n",
    "testN = valtestN - valN\n",
    "\n",
    "assert totalN == trainN + valN + testN\n",
    "\n",
    "lengths = [trainN, valN, testN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a86f34d-4609-4981-9d3f-7a5c79c6f5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/12/2024 11:08:18 - INFO - __main__ -   # lengths: train [1462673]  valid [487558]  test [487558]\n",
      "09/12/2024 11:08:18 - INFO - __main__ -   # lengths: train [0.60]  valid [0.20]  test [0.20]\n"
     ]
    }
   ],
   "source": [
    "log.info(f\"# lengths: train [{trainN}]  valid [{valN}]  test [{testN}]\")\n",
    "log.info(\"# lengths: train [{:.2f}]  valid [{:.2f}]  test [{:.2f}]\".format(trainN / totalN, valN / totalN,\n",
    "                                                                           testN / totalN))\n",
    "\n",
    "train_dataset, eval_dataset, test_dataset = random_split_dataset(dataset, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01f2aa90-2eb6-4a51-90b2-b86c4296a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dataset:\n",
    "    batch = i\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ef4bf5-491c-4528-a000-8c63682670a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  1388,   2007,   2016,   2035,   2036,   8846, 102579, 115840, 117412,\n",
       "          143360, 143466],\n",
       "         [  1388,   2007,   2016,   2026,   2036,   5003, 102579, 115840, 117412,\n",
       "          143369, 143466],\n",
       "         [  1388,   2007,   2016,   2034,   2036,   2039, 102579, 115840, 117412,\n",
       "          143358, 143466],\n",
       "         [  1388,   2007,   2016,   2030,   2036,   2039, 102579, 115840, 117412,\n",
       "          143358, 143466],\n",
       "         [  1388,   2007,   2016,   2028,   2036,   2039, 102579, 115840, 117412,\n",
       "          143358, 143466],\n",
       "         [  1388,   2007,   2016,   2033,   2036,  23500, 102579, 115840, 117412,\n",
       "          143374, 143466],\n",
       "         [  1388,   2007,   2016,   2030,   2036,   2062, 102579, 115840, 124395,\n",
       "          143368, 143466],\n",
       "         [  1388,   2007,   2016,   2026,   2036,   2041, 102579, 115840, 117412,\n",
       "          143359, 143466],\n",
       "         [  1388,   2007,   2016,   2030,   2036,   2041, 102579, 115840, 117412,\n",
       "          143359, 143466],\n",
       "         [  1388,   2007,   2016,   2034,   2036,   2041, 102579, 115840, 117412,\n",
       "          143359, 143466]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "668e86b4-3aa9-4be6-8ea6-1609d9a0a3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers.models.bert import BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import ACT2FN\n",
    "from transformers.models.bert.modeling_bert import BertForMaskedLM\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from models.custom_criterion import CustomAdaptiveLogSoftmax\n",
    "\n",
    "BertLayerNorm = torch.nn.LayerNorm\n",
    "\n",
    "class TabFormerBertConfig(BertConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        flatten=True,\n",
    "        ncols=12,\n",
    "        vocab_size=30522,\n",
    "        field_hidden_size=64,\n",
    "        hidden_size=768,\n",
    "        num_attention_heads=12,\n",
    "        pad_token_id=0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(pad_token_id=pad_token_id, **kwargs)\n",
    "\n",
    "        self.ncols = ncols\n",
    "        self.field_hidden_size = field_hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.flatten = flatten\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_attention_heads=num_attention_heads\n",
    "\n",
    "class TabFormerBertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.field_hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class TabFormerBertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = TabFormerBertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "class TabFormerBertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = TabFormerBertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "\n",
    "class TabFormerBertForMaskedLM(BertForMaskedLM):\n",
    "    def __init__(self, config, vocab):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.cls = TabFormerBertOnlyMLMHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            masked_lm_labels=None,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            lm_labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]  # [bsz * seqlen * hidden]\n",
    "\n",
    "        if not self.config.flatten:\n",
    "            output_sz = list(sequence_output.size())\n",
    "            expected_sz = [output_sz[0], output_sz[1]*self.config.ncols, -1]\n",
    "            sequence_output = sequence_output.view(expected_sz)\n",
    "            masked_lm_labels = masked_lm_labels.view(expected_sz[0], -1)\n",
    "\n",
    "        prediction_scores = self.cls(sequence_output) # [bsz * seqlen * vocab_sz]\n",
    "\n",
    "        outputs = (prediction_scores,) + outputs[2:]\n",
    "\n",
    "        # prediction_scores : [bsz x seqlen x vsz]\n",
    "        # masked_lm_labels  : [bsz x seqlen]\n",
    "\n",
    "        total_masked_lm_loss = 0\n",
    "\n",
    "        seq_len = prediction_scores.size(1)\n",
    "        # TODO : remove_target is True for card\n",
    "        field_names = self.vocab.get_field_keys(remove_target=True, ignore_special=False)\n",
    "        for field_idx, field_name in enumerate(field_names):\n",
    "            col_ids = list(range(field_idx, seq_len, len(field_names)))\n",
    "\n",
    "            global_ids_field = self.vocab.get_field_ids(field_name)\n",
    "\n",
    "            prediction_scores_field = prediction_scores[:, col_ids, :][:, :, global_ids_field]  # bsz * 10 * K\n",
    "            masked_lm_labels_field = masked_lm_labels[:, col_ids]\n",
    "            masked_lm_labels_field_local = self.vocab.get_from_global_ids(global_ids=masked_lm_labels_field,\n",
    "                                                                          what_to_get='local_ids')\n",
    "\n",
    "            nfeas = len(global_ids_field)\n",
    "            loss_fct = self.get_criterion(field_name, nfeas, prediction_scores.device)\n",
    "\n",
    "            masked_lm_loss_field = loss_fct(prediction_scores_field.view(-1, len(global_ids_field)),\n",
    "                                            masked_lm_labels_field_local.view(-1))\n",
    "\n",
    "            total_masked_lm_loss += masked_lm_loss_field\n",
    "\n",
    "        return (total_masked_lm_loss,) + outputs\n",
    "\n",
    "    def get_criterion(self, fname, vs, device, cutoffs=False, div_value=4.0):\n",
    "\n",
    "        if fname in self.vocab.adap_sm_cols:\n",
    "            if not cutoffs:\n",
    "                cutoffs = [int(vs/15), 3*int(vs/15), 6*int(vs/15)]\n",
    "\n",
    "            criteria = CustomAdaptiveLogSoftmax(in_features=vs, n_classes=vs, cutoffs=cutoffs, div_value=div_value)\n",
    "\n",
    "            return criteria.to(device)\n",
    "        else:\n",
    "            return CrossEntropyLoss()\n",
    "\n",
    "class TabFormerBertModel(BertForMaskedLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.cls = TabFormerBertOnlyMLMHead(config)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            token_type_ids=None,\n",
    "            position_ids=None,\n",
    "            head_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            masked_lm_labels=None,\n",
    "            encoder_hidden_states=None,\n",
    "            encoder_attention_mask=None,\n",
    "            lm_labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]  # [bsz * seqlen * hidden]\n",
    "\n",
    "        return sequence_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e01ffaa5-636e-4ec0-bed5-1ee0f9a3d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabFormerBertLM:\n",
    "    def __init__(self, special_tokens, vocab, field_ce=False, flatten=False, ncols=None, field_hidden_size=768):\n",
    "\n",
    "        self.ncols = ncols\n",
    "        self.vocab = vocab\n",
    "        vocab_file = self.vocab.filename\n",
    "        hidden_size = field_hidden_size if flatten else (field_hidden_size * self.ncols)\n",
    "\n",
    "        self.config = TabFormerBertConfig(vocab_size=len(self.vocab),\n",
    "                                          ncols=self.ncols,\n",
    "                                          hidden_size=hidden_size,\n",
    "                                          field_hidden_size=field_hidden_size,\n",
    "                                          flatten=flatten,\n",
    "                                          num_attention_heads=self.ncols)\n",
    "\n",
    "        self.tokenizer = BertTokenizer(vocab_file,\n",
    "                                       do_lower_case=False,\n",
    "                                       **special_tokens)\n",
    "        self.model = self.get_model(field_ce, flatten)\n",
    "\n",
    "    def get_model(self, field_ce, flatten):\n",
    "\n",
    "        model = TabFormerBertForMaskedLM(self.config, self.vocab)\n",
    "        # if flatten and not field_ce:\n",
    "        #     # flattened vanilla BERT\n",
    "        #     model = BertForMaskedLM(self.config)\n",
    "        # elif flatten and field_ce:\n",
    "        #     # flattened field CE BERT\n",
    "        #     model = TabFormerBertForMaskedLM(self.config, self.vocab)\n",
    "        # else:\n",
    "        #     # hierarchical field CE BERT\n",
    "        #     model = TabFormerHierarchicalLM(self.config, self.vocab)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe70abc6-f4f1-4a1f-bd31-338d0471f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "\n",
    "class TabFormerTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unk_token=\"<|endoftext|>\",\n",
    "        bos_token=\"<|endoftext|>\",\n",
    "        eos_token=\"<|endoftext|>\",\n",
    "    ):\n",
    "\n",
    "        super().__init__(bos_token=bos_token, eos_token=eos_token, unk_token=unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ee743e-d9dd-4124-a3e4-fec137a8509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_net = TabFormerBertLM(custom_special_tokens,\n",
    "                               vocab=vocab,\n",
    "                               # field_ce=args.field_ce,\n",
    "                               # flatten=args.flatten,\n",
    "                               ncols=dataset.ncols,\n",
    "                               # field_hidden_size=args.field_hs\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ccf478d-084b-4e12-9859-a8e83087d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tab_net.get_model(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43d31b76-0638-49a1-bcff-2de609d9a8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = eval(\"DataCollatorForLanguageModeling\")(\n",
    "        tokenizer=tab_net.tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e8b92aa-2c19-4e94-afe7-c1a8cd8337c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments('out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb77ba65-bada-409a-a007-9895315e3e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/12/2024 11:12:01 - WARNING - accelerate.utils.other -   Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=tab_net.model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "986dbf83-209f-485b-a050-b897a6d89ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "epochs = 4\n",
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "train_acc_ = []\n",
    "test_acc_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65dbf9ad-899e-456b-839b-1b7be38cb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8156002b-6078-43e0-94f8-f7efa8f8cbf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find a valid checkpoint at models/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/my_pyg/lib/python3.12/site-packages/transformers/trainer.py:1857\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resume_from_checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_enabled:\n\u001b[0;32m-> 1857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_from_checkpoint(resume_from_checkpoint)\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;66;03m# In case of repeating the find_executable_batch_size, set `self._train_batch_size` properly\u001b[39;00m\n\u001b[1;32m   1859\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrainerState\u001b[38;5;241m.\u001b[39mload_from_json(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(resume_from_checkpoint, TRAINER_STATE_NAME))\n",
      "File \u001b[0;32m~/.conda/envs/my_pyg/lib/python3.12/site-packages/transformers/trainer.py:2459\u001b[0m, in \u001b[0;36mTrainer._load_from_checkpoint\u001b[0;34m(self, resume_from_checkpoint, model)\u001b[0m\n\u001b[1;32m   2442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_from_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is only supported when using PyTorch FSDP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   2446\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(f)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m adapter_subdirs\n\u001b[1;32m   2458\u001b[0m ):\n\u001b[0;32m-> 2459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a valid checkpoint at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_from_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2461\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_from_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(config_file):\n",
      "\u001b[0;31mValueError\u001b[0m: Can't find a valid checkpoint at models/"
     ]
    }
   ],
   "source": [
    "trainer.train(model_path='models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b5da0-c8df-4fb4-a38a-93f4147e31cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1462673 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_labels)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# model.hidden = model.init_hidden()\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m output \u001b[38;5;241m=\u001b[39m model(train_inputs\u001b[38;5;241m.\u001b[39mt())\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, (train_labels))\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/my_pyg/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/my_pyg/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 116\u001b[0m, in \u001b[0;36mTabFormerBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, masked_lm_labels, encoder_hidden_states, encoder_attention_mask, lm_labels)\u001b[0m\n\u001b[1;32m    114\u001b[0m     expected_sz \u001b[38;5;241m=\u001b[39m [output_sz[\u001b[38;5;241m0\u001b[39m], output_sz[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mncols, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    115\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m sequence_output\u001b[38;5;241m.\u001b[39mview(expected_sz)\n\u001b[0;32m--> 116\u001b[0m     masked_lm_labels \u001b[38;5;241m=\u001b[39m masked_lm_labels\u001b[38;5;241m.\u001b[39mview(expected_sz[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    118\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output) \u001b[38;5;66;03m# [bsz * seqlen * vocab_sz]\u001b[39;00m\n\u001b[1;32m    120\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (prediction_scores,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "        optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        ## training epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        for iter, traindata in enumerate(tqdm.tqdm(train_dataset)):\n",
    "            # print(iter)\n",
    "            train_inputs, train_labels = traindata\n",
    "            train_labels = torch.squeeze(train_labels)\n",
    "\n",
    "            if use_gpu:\n",
    "                train_inputs, train_labels = (train_inputs.cuda()), train_labels.cuda()\n",
    "            # else: train_inputs = (train_inputs)\n",
    "\n",
    "            model.zero_grad()\n",
    "            model.batch_size = len(train_labels)\n",
    "            # model.hidden = model.init_hidden()\n",
    "            output = model(train_inputs.t())\n",
    "\n",
    "            loss = loss_function(output, (train_labels))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calc training acc\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_acc += (predicted == train_labels).sum()\n",
    "            total += len(train_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss_.append(total_loss / total)\n",
    "        train_acc_.append(total_acc / total)\n",
    "        ## testing epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        for iter, testdata in enumerate(tqdm.tqdm(test_dataset)):\n",
    "            test_inputs, test_labels = testdata\n",
    "            test_labels = torch.squeeze(test_labels)\n",
    "\n",
    "            if use_gpu:\n",
    "                test_inputs, test_labels = (test_inputs.cuda()), test_labels.cuda()\n",
    "            else: test_inputs = Variable(test_inputs)\n",
    "\n",
    "            model.batch_size = len(test_labels)\n",
    "            # model.hidden = model.init_hidden()\n",
    "            output = model(test_inputs.t())\n",
    "\n",
    "            loss = loss_function(output, (test_labels))\n",
    "\n",
    "            # calc testing acc\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_acc += (predicted == test_labels).sum()\n",
    "            total += len(test_labels)\n",
    "            total_loss += loss.item()\n",
    "        test_loss_.append(total_loss / total)\n",
    "        test_acc_.append(total_acc / total)\n",
    "\n",
    "        print('[Epoch: %3d/%3d] Training Loss: %.6f, Testing Loss: %.6f, Training Acc: %.6f, Testing Acc: %.6f'\n",
    "              % (epoch, epochs, train_loss_[epoch], test_loss_[epoch], train_acc_[epoch], test_acc_[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fa30bcd-19c0-44eb-8075-24be31d5d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 487558/487558 [04:07<00:00, 1966.78it/s]\n"
     ]
    }
   ],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "for iter, testdata in enumerate(tqdm.tqdm(test_dataset)):\n",
    "            test_inputs, test_labels = testdata\n",
    "            test_labels = torch.squeeze(test_labels)\n",
    "\n",
    "            if use_gpu:\n",
    "                test_inputs, test_labels = Variable(test_inputs.cuda()), test_labels.cuda()\n",
    "            else: test_inputs = Variable(test_inputs)\n",
    "\n",
    "            model.batch_size = len(test_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(test_inputs.t())\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            \n",
    "            y_pred.append(predicted.tolist())\n",
    "            y_test.append(test_labels.reshape(-1).tolist())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0157062c-13be-4e0d-92f9-cc4c3b6ff4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7758690379951495"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3839/(3839 + 0.5*(1917+301))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b093a843-e111-4c69-8605-1aa67989837e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "cm = confusion_matrix(np.asarray(y_test).reshape(-1), np.asarray(y_pred).reshape(-1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad0eaca8-5032-4c35-ac50-08cf8211c70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99977231, 0.77586904])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(np.asarray(y_test).reshape(-1), np.asarray(y_pred).reshape(-1), average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46bd9415-a1d6-43e4-9851-be8b494d72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "epochs = 4\n",
    "batch_size = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "train_acc_ = []\n",
    "test_acc_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfbb00b-c85b-4e35-81bd-0b8fd2192e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 770328/1462673 [17:00<17:20, 665.28it/s]  IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 13%|█▎        | 189416/1462673 [04:05<32:51, 645.98it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "        optimizer = adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        ## training epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        for iter, traindata in enumerate(tqdm.tqdm(train_dataset)):\n",
    "            # print(iter)\n",
    "            train_inputs, train_labels = traindata\n",
    "            train_labels = torch.squeeze(train_labels)\n",
    "\n",
    "            if use_gpu:\n",
    "                train_inputs, train_labels = Variable(train_inputs.cuda()), train_labels.cuda()\n",
    "            else: troutputn_inputs = Variable(train_inputs)\n",
    "\n",
    "            model.zero_grad()\n",
    "            model.batch_size = len(train_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(train_inputs.t())\n",
    "\n",
    "            loss = loss_function(output, Variable(train_labels))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calc training acc\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_acc += (predicted == train_labels).sum()\n",
    "            total += len(train_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss_.append(total_loss / total)\n",
    "        train_acc_.append(total_acc / total)\n",
    "        ## testing epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        for iter, testdata in enumerate(tqdm.tqdm(test_dataset)):\n",
    "            test_inputs, test_labels = testdata\n",
    "            test_labels = torch.squeeze(test_labels)\n",
    "\n",
    "            if use_gpu:\n",
    "                test_inputs, test_labels = Variable(test_inputs.cuda()), test_labels.cuda()\n",
    "            else: test_inputs = Variable(test_inputs)\n",
    "\n",
    "            model.batch_size = len(test_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(test_inputs.t())\n",
    "\n",
    "            loss = loss_function(output, Variable(test_labels))\n",
    "\n",
    "            # calc testing acc\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_acc += (predicted == test_labels).sum()\n",
    "            total += len(test_labels)\n",
    "            total_loss += loss.item()\n",
    "        test_loss_.append(total_loss / total)\n",
    "        test_acc_.append(total_acc / total)\n",
    "\n",
    "        print('[Epoch: %3d/%3d] Training Loss: %.6f, Testing Loss: %.6f, Training Acc: %.6f, Testing Acc: %.6f'\n",
    "              % (epoch, epochs, train_loss_[epoch], test_loss_[epoch], train_acc_[epoch], test_acc_[epoch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbdd6b-c361-4d0b-aa72-68c5ec5a36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "for iter, testdata in enumerate(tqdm.tqdm(test_dataset)):\n",
    "            test_inputs, test_labels = testdata\n",
    "            test_labels = torch.squeeze(test_labels)\n",
    "\n",
    "            if use_gpu:\n",
    "                test_inputs, test_labels = Variable(test_inputs.cuda()), test_labels.cuda()\n",
    "            else: test_inputs = Variable(test_inputs)\n",
    "\n",
    "            model.batch_size = len(test_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(test_inputs.t())\n",
    "            \n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            \n",
    "            y_pred.append(predicted.tolist())\n",
    "            y_test.append(test_labels.reshape(-1).tolist())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa65de-f607-4887-b15d-e573cd09018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(np.asarray(y_test).reshape(-1), np.asarray(y_pred).reshape(-1), average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0c292-eb56-47b4-8460-1886185f2a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-my_pyg]",
   "language": "python",
   "name": "conda-env-.conda-my_pyg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
